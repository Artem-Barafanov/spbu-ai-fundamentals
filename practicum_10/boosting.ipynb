{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier, XGBRegressor, XGBRFRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "SEED = 314159\n",
    "TRAIN_TEST_SPLIT = 0.80\n",
    "\n",
    "data_path = r\"C:\\Users\\nikol_ri8fhbe\\Documents\\ml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, \n",
    "                           n_redundant=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 2], X[:, 3], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Ансамбли: бустинги\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Бустинг строится последовательно: каждое следующее дерево в нем обучается на основе результатов предыдущего, пытаясь уменьшить его ошибку. Как следствие, композиция будет иметь меньшее смещение, чем исходные базовые модели. Поэтому логично в качестве базовых моделей использовать те, которые изначально обладают небольшим разбросом и высоким смещением. Вопрос: какие это будут модели?\n",
    "Еще одно соображение для выбора сильно смещенных моделей в том, что они банально быстрее учатся. Так как невозможно распараллелить обучение базовых моделей, то скорость их настройки становится серьезным вопросом. \n",
    "\n",
    "Что интересно, бустинги не очень хорошо работают с однородными данными - поэтому их нечасто применяют для текстов.\n",
    "\n",
    "Расссмотрим квадратичную функцию потерь и композицию следующего вида: $ a = b_1 +  b_2 + ... + b_N $\n",
    "Обучим только одно дерево $ a = b_1 $. Найдем примеры, для которых оно ошибается в  предсказании. Обучим для них еще одно дерево - $ b_2 $, которое будет предсказывать ошибку первого. Будем повторять это, пока не наберем K деревьев. Примерно так на верхнем уровне обучается бустинг. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "\n",
    "def eval_classifier(clf):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=43)\n",
    "    n_scores = cross_val_score(clf, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return np.mean(n_scores), np.std(n_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_accuracy = pd.DataFrame(0.0,\n",
    "                                columns=[\"W/o ensembling\", 'Bagging', \"Bagging_with_mf\", 'AdaBoost'],\n",
    "                                index=['deep DTC', '1-level DTC', 'LR', 'SVC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(DecisionTreeClassifier())\n",
    "\n",
    "results_accuracy.loc['deep DTC', 'W/o ensembling'] = acc_mean\n",
    "print(f\"{acc_mean:.2f}, +- {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(DecisionTreeClassifier(max_depth=1))\n",
    "\n",
    "results_accuracy.loc['1-level DTC', 'W/o ensembling'] = acc_mean\n",
    "print(f\"{acc_mean:.2f} +- {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(\n",
    "    BaggingClassifier(DecisionTreeClassifier(), n_estimators=10, \n",
    "                      max_samples=1.0, max_features=1.0, \n",
    "                      bootstrap=True, bootstrap_features=False))\n",
    "\n",
    "results_accuracy.loc['deep DTC', 'Bagging'] = acc_mean\n",
    "print(f\"{acc_mean:.2f}, {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(\n",
    "    BaggingClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=10, \n",
    "                      max_samples=1.0, max_features=1.0, \n",
    "                      bootstrap=True, bootstrap_features=False))\n",
    "\n",
    "results_accuracy.loc['1-level DTC', 'Bagging'] = acc_mean\n",
    "print(f\"{acc_mean:.2f}, {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(\n",
    "    BaggingClassifier(DecisionTreeClassifier(), n_estimators=10, \n",
    "                      max_samples=1.0, max_features=0.8, \n",
    "                      bootstrap=True, bootstrap_features=False))\n",
    "\n",
    "results_accuracy.loc['deep DTC', 'Bagging_with_mf'] = acc_mean\n",
    "print(f\"{acc_mean:.2f}, {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(\n",
    "    BaggingClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=10, \n",
    "                      max_samples=1.0, max_features=0.8, \n",
    "                      bootstrap=True, bootstrap_features=False))\n",
    "\n",
    "results_accuracy.loc['1-level DTC', 'Bagging_with_mf'] = acc_mean\n",
    "print(f\"{acc_mean:.2f}, {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Дополните таблицу: обучите также логистическую регрессию с беггингом и без него."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(LogisticRegression(max_iter=1000))\n",
    "\n",
    "results_accuracy.loc['LR', 'W/o ensembling'] = acc_mean\n",
    "print(f\"LR W/o ensembling: {acc_mean:.2f}, +- {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(\n",
    "    BaggingClassifier(LogisticRegression(max_iter=1000), n_estimators=10,\n",
    "    max_samples=1.0, max_features=1.0,\n",
    "    bootstrap=True, bootstrap_features=False))\n",
    "\n",
    "results_accuracy.loc['LR', 'Bagging'] = acc_mean\n",
    "print(f\"LR with Bagging: {acc_mean:.2f}, +- {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(\n",
    "    BaggingClassifier(LogisticRegression(max_iter=1000), n_estimators=10,\n",
    "    max_samples=1.0, max_features=0.8,\n",
    "    bootstrap=True, bootstrap_features=False))\n",
    "\n",
    "results_accuracy.loc['LR', 'Bagging_with_mf'] = acc_mean\n",
    "print(f\"LR with Bagging_with_mf: {acc_mean:.2f}, +- {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost обучает каждый следующий классификатор на объектах, на которых ошибаются предыдущие (объекты с ошибками получают больший вес, без ошибок — меньший)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(\n",
    "    AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=50, learning_rate=1.0))\n",
    "\n",
    "results_accuracy.loc['deep DTC', 'AdaBoost'] = acc_mean\n",
    "print(f\"{acc_mean:.2f}, {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(\n",
    "    AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=50, learning_rate=1.0))\n",
    "\n",
    "results_accuracy.loc['1-level DTC', 'AdaBoost'] = acc_mean\n",
    "print(f\"{acc_mean:.2f}, {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(s, props=''):\n",
    "    return np.where(s == np.nanmax(s.values), props, '')\n",
    "\n",
    "results_to_show = results_accuracy.copy()\n",
    "\n",
    "results_to_show.style.apply(highlight_max, props='font-weight: bold;', axis=1).format('{:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание:** выясните, дадут ли улучшение бэггинг и бустинг над линейной регрессией. Объясните, почему так."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Наверное, в задании имелась в виду логистическая регрессия, так как наш датасет создан как задача классификации. Поэтому добавим LR AdaBoost в табличку и оценим результаты._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(\n",
    "    AdaBoostClassifier(\n",
    "        estimator=LogisticRegression(max_iter=1000),\n",
    "        n_estimators=50,\n",
    "        learning_rate=1.0,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "results_accuracy.loc['LR', 'AdaBoost'] = acc_mean\n",
    "print(f\"LR with AdaBoost: {acc_mean:.2f}, ± {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_show = results_accuracy.copy()\n",
    "\n",
    "results_to_show.style.apply(highlight_max, props='font-weight: bold;', axis=1).format('{:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Возможное объяснение результатов**:_ \n",
    "\n",
    "_Логистическая регрессия - линейная модель с низкой дисперсией. Бэггинг помогает уменьшать дисперсию, но у линейных моделей она и так минимальна. Усреднение множества линейных моделей дает такую же линейную модель._\n",
    "\n",
    "_А что касается AdaBoost, он предназначен для слабых классификаторов (мелкие деревья), а не для линейных моделей, а логистическая регрессия и так дает оптимальное линейное решение. Поэтому попытки \"бустить\" линейную модель приводят к переобучению или ухудшению качества._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вопрос: Почему AdaBoost хуже работает на глубоких деревьях?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Возможный ответ:**_\n",
    "\n",
    "_Идея бустинга - это последовательное улучшение за счет исправления ошибок простых моделей. А глубокие деревья — уже сильные классификаторы, а потому на бустинге они вполне спокойно могут переобучаться._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Градиентный бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Рассмотрим композицию $a = \\sum_{i} {\\gamma_i b_i}$. Для начала выбирается какой-нибудь простой $\\gamma_0, b_0$ (например, 0 и среднее). Формально каждая N-ая модель, начиная со второй, пытается приблизиить антиградиент функционала ошибки, взятый в точках ${z_i=a_{N-1}(x_i)}$:\n",
    "$$s_i = -\\dfrac{\\partial L(y, z)}{\\partial z}|_{z=a_{N-1}}$$\n",
    "Подбор алгоритма при этом производится, приближая эту ошибку c точки зрения квадратичной функции потерь.\n",
    "$$ b_i = arg \\min_{b \\in \\mathcal{B}} {\\sum (b_i(x) - s_i)^2} $$\n",
    "\n",
    "Градиентный бустинг - довольно мощная метамодель, с огромным количеством параметров и хитростей. Мы сегодня остановимся только на основных. Для начала рассмотрим самый стандартный бустинг с использованием деревьев решений (CART). Параметры базовых моделей такие же, как и раньше, но настройка амого бустинга довольно сложна!\n",
    "\n",
    "Важный вопрос при обучении модели - какую функцию ошибок выбрать? Какая задача возникает при обработке датасета с вином?\n",
    "\n",
    "Для того, чтобы оценивать модель, полезны различные метрики - численные характеристики ее качества. При этом бустинги настолько галантны, что предоставляют нам возможность оценивать метрики прямо при обучении. Для этого необходимо задать тип метрики в конструкторе и eval_set при запуске fit()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "На практике обычно используется один из трех вариантов бустинга - Xgboost, LightGBM или CatBoost.\n",
    "\n",
    "### [XGBoost](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf).\n",
    "Плюсы:\n",
    "- Позволяет легко паралелить вычисления (например на спарке)\n",
    "- Легко использовать с sklearn и numpy (но с потерей производительности)\n",
    "- Поддерживается обработка разреженных данных\n",
    "- Предсортированные блоки, кэши, шардирование\n",
    "\n",
    "Минусы:\n",
    "- Нет поддержки GPU\n",
    "\n",
    "[документация](https://xgboost.readthedocs.io/en/latest/)\n",
    "\n",
    "  \n",
    "### [LightGBM](https://papers.nips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf)\n",
    "Плюсы:\n",
    "- Поддержка GPU\n",
    "- Метод Фишера для работы с категориальными признаками\n",
    "- Уменьшение размера обучающей выборки (GOSS)\n",
    "- Объединение разреженных признаков (EFB)\n",
    "\n",
    "Минусы:\n",
    "- Итерфейс не совместим с sklearn/numpy\n",
    "\n",
    "[документация](https://lightgbm.readthedocs.io/en/latest/Python-API.html)\n",
    "\n",
    "### [CatBoost](https://papers.nips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf)\n",
    "Плюсы:\n",
    "- Поддержка GPU\n",
    "- Легко использовать с sklearn и numpy\n",
    "- Более продвинутая работа с категориальными фичами\n",
    "- Наши слоны\n",
    "  \n",
    "Минусы:\n",
    "- Бывает работает хуже (возможно слабее эвристики), но с категориальными фичами — хорошо\n",
    "\n",
    "[документация](https://catboost.ai/docs/concepts/python-quickstart.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "acc_mean, acc_std = eval_classifier(\n",
    "    CatBoostClassifier(\n",
    "        iterations=10,\n",
    "        depth=1,\n",
    "        learning_rate=1,\n",
    "        loss_function='Logloss',\n",
    "        verbose=True, \n",
    "        task_type='CPU'))\n",
    "\n",
    "сat_boost = acc_mean\n",
    "\n",
    "results_accuracy.loc['1-level DTC', 'CatBoost'] = acc_mean\n",
    "print(f\"{acc_mean:.2f}, {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "acc_mean, acc_std = eval_classifier(\n",
    "    CatBoostClassifier(\n",
    "        iterations=10,\n",
    "        learning_rate=1,\n",
    "        loss_function='Logloss',\n",
    "        verbose=True, \n",
    "        task_type='CPU'))\n",
    "\n",
    "сat_boost = acc_mean\n",
    "\n",
    "results_accuracy.loc['deep DTC', 'CatBoost'] = acc_mean\n",
    "print(f\"{acc_mean:.2f}, {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "acc_mean, acc_std = eval_classifier(XGBClassifier(objective='binary:logistic', random_state=42))\n",
    "\n",
    "xg_boost = acc_mean\n",
    "print(f\"{acc_mean:.2f}, {acc_std:.2f}\")\n",
    "\n",
    "results_accuracy.loc['deep DTC', 'XGBoost'] = acc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(XGBClassifier(objective='binary:logistic', max_depth=1, random_state=42))\n",
    "\n",
    "xg_boost = acc_mean\n",
    "print(f\"{acc_mean:.2f}, {acc_std:.2f}\")\n",
    "\n",
    "results_accuracy.loc['1-level DTC', 'XGBoost'] = acc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_show = results_accuracy.copy()\n",
    "results_to_show.style.apply(highlight_max, props='font-weight: bold;', axis=1).format('{:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "train_data = lgb.Dataset(X, label=y)\n",
    "\n",
    "param = {'num_leaves': 31, \n",
    "         'objective': 'multiclass', \n",
    "         'num_class': 2, \n",
    "         'metric': ['multi_logloss']}\n",
    "\n",
    "num_round = 10\n",
    "boost = lgb.train(param, train_data, num_boost_round=10)\n",
    "\n",
    "lg_boost = (boost.predict(X).argmax(axis=-1) == y).mean()\n",
    "\n",
    "results_accuracy.loc['deep DTC', 'LightGBM'] = lg_boost\n",
    "print(f\"{lg_boost:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_show = results_accuracy.copy()\n",
    "\n",
    "results_to_show.style.apply(highlight_max, props='font-weight: bold;', axis=1).format('{:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример\n",
    "Рассмотрим реальный датасет, и на его примере попробуем поработать с бустингом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "ds = datasets.load_diabetes()\n",
    "X = ds.data\n",
    "Y = ds.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.5, test_size=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = XGBRegressor(n_estimators=100, learning_rate=1, seed=SEED)\n",
    "fit_params = {\"eval_set\":[(X_train, y_train),(X_test, y_test)], \"verbose\": False}\n",
    "# Add verbose=False to avoid printing out updates with each cycle\n",
    "model.fit(X_train, y_train,\n",
    "            eval_set=[(X_train, y_train),(X_test, y_test)],\n",
    "            verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "results = model.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "error_function = \"rmse\"\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(results[\"validation_0\"][error_function], label=\"Training loss\")\n",
    "plt.plot(results[\"validation_1\"][error_function], label=\"Validation loss\")\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Как мы видим, хотя лосс при обучении падал и падал, на валидации метрики перестали улучшаться довольно рано. Это довольно плохой знак. Однако говорит ли это о катастрофической ситуации? Проверим переобучение с помощью кросс-валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(model, X, Y, cv=10, scoring=[\"neg_root_mean_squared_error\"],\n",
    "                            return_train_score=True)\n",
    "print(\"Train RMSE is\", -cv_results['train_neg_root_mean_squared_error'].mean())\n",
    "print(\"Test RMSE is\", -cv_results['test_neg_root_mean_squared_error'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Кажется, у нас действительно серьезные проблемы. Попробуем уменьшить скорость обучения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# train and eval model with smaller lr\n",
    "model = XGBRegressor(n_estimators=100, learning_rate=0.01, seed=SEED)\n",
    "fit_params = {\"eval_set\":[(X_train, y_train),(X_test, y_test)], \"verbose\": False}\n",
    "# Add verbose=False to avoid printing out updates with each cycle\n",
    "model.fit(X_train, y_train,\n",
    "            eval_set=[(X_train, y_train),(X_test, y_test)],\n",
    "            verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "results = model.evals_result()\n",
    "error_function = \"rmse\"\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(results[\"validation_0\"][error_function], label=\"Training loss\")\n",
    "plt.plot(results[\"validation_1\"][error_function], label=\"Validation loss\")\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Помогло ли это? Попробуем получить результаты лучше, поиграв с параметрами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(model, X, Y, cv=10, scoring=[\"neg_root_mean_squared_error\"],\n",
    "                            return_train_score=True)\n",
    "print(\"Train RMSE is\", -cv_results['train_neg_root_mean_squared_error'].mean())\n",
    "print(\"Test RMSE is\", -cv_results['test_neg_root_mean_squared_error'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(params, X_train, y_train, X_test, y_test, X, Y):\n",
    "    label = params.pop(\"label\")\n",
    "    model = XGBRegressor(**params)\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    results = model.evals_result()\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(results[\"validation_0\"][\"rmse\"], label=\"Тренировочный\")\n",
    "    plt.plot(results[\"validation_1\"][\"rmse\"], label=\"Валидационный\")\n",
    "    plt.title(f\"Learning curve (params: {label})\")\n",
    "    plt.xlabel(\"Number of trees\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    model_cv = XGBRegressor(**params)\n",
    "    cv_results = cross_validate(\n",
    "        model_cv, X, Y,\n",
    "        cv=5,\n",
    "        scoring=[\"neg_root_mean_squared_error\"],\n",
    "        return_train_score=True\n",
    "    )\n",
    "    print(\"Train RMSE:\", -cv_results['train_neg_root_mean_squared_error'].mean())\n",
    "    print(\"Test RMSE:\", -cv_results['test_neg_root_mean_squared_error'].mean())\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "params1 = {\n",
    "    \"n_estimators\": 1000,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"max_depth\": 6,\n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"gamma\": 0,\n",
    "    \"reg_alpha\": 0.0,\n",
    "    \"reg_lambda\": 1.0,\n",
    "    \"random_state\": SEED,\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"label\": \"params1\"\n",
    "}\n",
    "train_and_evaluate(params1, X_train, y_train, X_test, y_test, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params2 = {\n",
    "    \"n_estimators\": 1000,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"max_depth\": 4,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"gamma\": 1,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 2.0,\n",
    "    \"random_state\": SEED,\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"label\": \"params2\"\n",
    "}\n",
    "train_and_evaluate(params2, X_train, y_train, X_test, y_test, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params3 = {\n",
    "    \"n_estimators\": 1000,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"max_depth\": 5,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"gamma\": 0.5,\n",
    "    \"reg_alpha\": 0.05,\n",
    "    \"reg_lambda\": 1.5,\n",
    "    \"random_state\": SEED,\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"label\": \"params3\"\n",
    "}\n",
    "train_and_evaluate(params3, X_train, y_train, X_test, y_test, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Так как параметров довольно много, может быть разумно автоматизировать их поиск. Для этого воспользуемся поиском по решетке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    seed=SEED,\n",
    "    verbosity=0\n",
    ")\n",
    "xgboost_params = {\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"subsample\": [0.7, 0.9],\n",
    "    \"colsample_bytree\": [0.7, 1.0],\n",
    "    \"reg_alpha\": [0, 0.1],\n",
    "    \"reg_lambda\": [1, 2],\n",
    "    \"n_estimators\": [300, 500]\n",
    "}\n",
    "fit_params = {\n",
    "    \"eval_set\": [(X_train, y_train), (X_test, y_test)],\n",
    "    \"verbose\": False\n",
    "}\n",
    "\n",
    "xgboost_best_grid = GridSearchCV(\n",
    "    model,\n",
    "    xgboost_params,\n",
    "    cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "xgboost_best_grid.fit(X_train, y_train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best RMSE:\", -xgboost_best_grid.best_score_)\n",
    "print(\"Best parameters:\")\n",
    "for k, v in xgboost_best_grid.best_params_.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Давайте проверим, какую точность мы получим с лучшими параметрами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "_Лучший результат показан в предыдущей ячейке._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание:** Проведите обучение и с LightGBM/CatBoost. Какие лучшие точности у вас получилось получить?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "lgb_model = LGBMRegressor(random_state=SEED)\n",
    "\n",
    "lgb_params = {\n",
    "    \"learning_rate\": [0.01, 0.05],\n",
    "    \"max_depth\": [3, 5],\n",
    "    \"n_estimators\": [300, 500],\n",
    "    \"subsample\": [0.8, 0.9],\n",
    "    \"colsample_bytree\": [0.8, 1.0],\n",
    "    \"reg_alpha\": [0, 0.1],\n",
    "    \"reg_lambda\": [1, 2]\n",
    "}\n",
    "\n",
    "lgb_grid = GridSearchCV(\n",
    "    lgb_model, lgb_params,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "lgb_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"LightGBM:\")\n",
    "print(\"Best RMSE:\", -lgb_grid.best_score_)\n",
    "print(\"Best parameters:\")\n",
    "for k, v in lgb_grid.best_params_.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Задание:** Постройте графики предсказаний для первых двух PCA фичей для бустингов разной глубины/разного числа деревьев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(\n",
    "    X_pca, Y, train_size=0.8, random_state=SEED\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_surface(model, X, y, title):\n",
    "    h = 0.1\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, cmap=\"coolwarm\", alpha=0.5)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"coolwarm\", edgecolors=\"k\", s=50)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"PCA 1\")\n",
    "    plt.ylabel(\"PCA 2\")\n",
    "    plt.colorbar(label=\"Предсказание\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for depth in [2, 4, 6]:\n",
    "    for n_trees in [100, 300]:\n",
    "        model = XGBRegressor(\n",
    "            max_depth=depth,\n",
    "            n_estimators=n_trees,\n",
    "            learning_rate=0.05,\n",
    "            random_state=SEED,\n",
    "            verbosity=0\n",
    "        )\n",
    "        model.fit(X_train_pca, y_train_pca)\n",
    "        title = f\"XGBoost (depth={depth}, trees={n_trees})\"\n",
    "        plot_prediction_surface(model, X_pca, Y, title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Постройте график зависимости точности от глубины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "rmse_scores = []\n",
    "\n",
    "for d in depths:\n",
    "    model = XGBRegressor(\n",
    "        max_depth=d,\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        random_state=SEED,\n",
    "        verbosity=0\n",
    "    )\n",
    "    scores = cross_val_score(model, X, Y,\n",
    "                              cv=5,\n",
    "                              scoring=\"neg_root_mean_squared_error\")\n",
    "    mean_rmse = -np.mean(scores)\n",
    "    rmse_scores.append(mean_rmse)\n",
    "    print(f\"depth={d}, RMSE={mean_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(depths, rmse_scores, marker=\"o\", linestyle=\"-\")\n",
    "plt.title(\"Зависимость RMSE от max_depth (XGBoost)\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
